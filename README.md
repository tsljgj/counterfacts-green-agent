<p align="center">
  <img src="https://img.shields.io/badge/Python-3.13+-blue.svg" alt="Python 3.13+">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
  <img src="https://img.shields.io/badge/A2A_Protocol-Compatible-purple.svg" alt="A2A Protocol">
  <img src="https://img.shields.io/badge/Docker-Ready-2496ED.svg" alt="Docker Ready">
</p>

# CounterFacts Green Agent

> **A benchmark evaluator for testing AI agents on multi-step reasoning questions**

The CounterFacts Green Agent is a benchmark evaluation service that tests AI agents on their ability to answer challenging, multi-step reasoning questions. It implements the [A2A (Agent-to-Agent) Protocol](https://a2a-protocol.org/) for interoperability with any compliant agent.

## About

This green agent uses the **AQA Benchmark Dataset** - a collection of curated, difficulty-tiered questions generated by the [AQAEnv](https://github.com/tsljgj/AQAEnv) pipeline. The dataset contains questions that require genuine multi-step reasoning, with verified ground truth answers across multiple difficulty levels.

### Key Features

- **167 Benchmark Questions** across 4 difficulty levels (easy, medium, hard, expert)
- **LLM-as-Judge Evaluation** using semantic equivalence checking (not just exact matching)
- **Difficulty-Weighted Scoring** - harder questions contribute more to the final score
- **Multi-Domain Coverage** - web search, physics, chemistry, biology, and more
- **A2A Protocol Compliant** - works with any A2A-compatible agent
- **Docker Ready** - build and deploy with a single command

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                    AgentBeats Platform / Client                     │
└──────────────────────────────┬──────────────────────────────────────┘
                               │ Assessment Request (A2A)
                               ▼
┌─────────────────────────────────────────────────────────────────────┐
│                COUNTERFACTS GREEN AGENT (This Repo)                 │
│  ┌───────────────────────────────────────────────────────────────┐  │
│  │  1. Parse request (num_tasks, difficulties, seed)             │  │
│  │  2. Sample questions from AQA dataset                         │  │
│  │  3. For each question:                                        │  │
│  │     └─ Send to purple agent → Receive answer → Evaluate       │  │
│  │  4. Compute metrics (pass rate, weighted score, latency)      │  │
│  │  5. Return structured results                                 │  │
│  └───────────────────────────────────────────────────────────────┘  │
└──────────────────────────────┬──────────────────────────────────────┘
                               │ Questions (A2A)
                               ▼
┌─────────────────────────────────────────────────────────────────────┐
│                    PURPLE AGENT (Being Tested)                      │
│                    Any A2A-compliant QA agent                       │
└─────────────────────────────────────────────────────────────────────┘
```

---

## Quick Start

### Prerequisites

- [Docker](https://docs.docker.com/get-docker/) (recommended) or Python 3.13+
- [uv](https://github.com/astral-sh/uv) package manager (for local development)
- OpenAI API key (for LLM-based answer evaluation)

### Option 1: Run with Docker (Recommended)

```bash
# Build the image
docker build -t counterfacts-green-agent .

# Run the container
docker run -p 9009:9009 -e OPENAI_API_KEY="your-key-here" counterfacts-green-agent
```

The agent will be available at `http://localhost:9009`.

### Option 2: Run Locally

```bash
# Clone the repository
git clone https://github.com/tsljgj/counterfacts-green-agent.git
cd counterfacts-green-agent

# Install dependencies
uv sync

# Set your OpenAI API key
export OPENAI_API_KEY="your-key-here"

# Run the server
uv run src/server.py
```

### Verify It's Running

```bash
# Check the agent card
curl http://localhost:9009/.well-known/agent-card.json
```

---

## Usage

### Assessment Request Format

Send a JSON request to the green agent with the following structure:

```json
{
  "participants": {
    "agent": "http://purple-agent:9009"
  },
  "config": {
    "num_tasks": 10,
    "difficulty": ["easy", "medium", "hard", "expert"],
    "seed": 42,
    "timeout_per_question": 30,
    "evaluator_model": "gpt-4o-mini",
    "domains": ["Web Search", "Physics"]
  }
}
```

#### Configuration Options

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `participants.agent` | URL | Yes | - | URL of the purple agent to test |
| `config.num_tasks` | int | Yes | - | Number of questions to ask |
| `config.difficulty` | list | No | All levels | Filter by difficulty: `easy`, `medium`, `hard`, `expert` |
| `config.seed` | int | No | Random | Seed for reproducible question sampling |
| `config.timeout_per_question` | int | No | 30 | Timeout per question in seconds |
| `config.evaluator_model` | str | No | `gpt-4o-mini` | OpenAI model for answer evaluation |
| `config.domains` | list | No | All | Filter by domain (e.g., `Web Search`, `Physics`) |

### Response Format

The green agent returns structured results including:

```json
{
  "assessment_id": "aqa-1736899200",
  "config": { ... },
  "items": [
    {
      "qid": "20260103_143526_level2",
      "difficulty": "hard",
      "question": "What is the atomic number of the element...",
      "reference_answer": "79",
      "agent_answer": "79 (Gold)",
      "correct": true,
      "score": 1.0,
      "latency_ms": 2500,
      "evaluation_reasoning": "Semantically equivalent answer"
    }
  ],
  "aggregate": {
    "total_tasks": 10,
    "correct": 7,
    "pass_rate": 0.7,
    "weighted_score": 0.65,
    "easy_accuracy": 1.0,
    "medium_accuracy": 0.75,
    "hard_accuracy": 0.5,
    "expert_accuracy": 0.25,
    "by_difficulty": { ... }
  }
}
```

---

## Testing Locally

This repository includes a sample **test purple agent** for end-to-end testing.

### Using Docker Compose (Recommended)

The easiest way to test locally is with Docker Compose, which starts both agents:

```bash
# Set your OpenAI API key
export OPENAI_API_KEY="your-key-here"

# Start both green and purple agents
docker compose up

# In another terminal, run the assessment
uv run test_interaction.py --num-tasks 5
```

### Manual Testing with the Included Purple Agent

1. Start the test purple agent on port 9010:
   ```bash
   # In terminal 1
   cd test-purple-agent
   export OPENAI_API_KEY="your-key-here"
   uv sync
   uv run src/server.py --port 9010
   ```

2. Start the green agent on port 9009:
   ```bash
   # In terminal 2
   export OPENAI_API_KEY="your-key-here"
   uv run src/server.py
   ```

3. Run the test script:
   ```bash
   # In terminal 3
   export OPENAI_API_KEY="your-key-here"
   uv run test_interaction.py --num-tasks 5 --difficulty easy medium
   ```

### Running Unit Tests

```bash
# Install test dependencies
uv sync --extra test

# Start the agent in one terminal
uv run src/server.py

# Run tests in another terminal
uv run pytest --agent-url http://localhost:9009
```

---

## Dataset

The AQA benchmark dataset contains **167 questions** across 4 difficulty levels:

| Difficulty | Questions | Weight | Description |
|------------|-----------|--------|-------------|
| Easy | 54 | 1x | Single-step reasoning |
| Medium | 47 | 2x | Two-step reasoning chains |
| Hard | 45 | 3x | Three-step reasoning chains |
| Expert | 21 | 4x | Four-step complex reasoning |

### Domain Distribution

- **Web Search**: 98 questions (58.7%) - fact-based questions requiring web search
- **Physics**: 18 questions - physics calculations and concepts
- **Chemistry**: 12 questions - chemical properties and reactions
- **Biology**: 10 questions - biological facts and processes
- **Other**: 29 questions - statistics, science, general knowledge

### Data Source

Questions are generated by the [AQAEnv](https://github.com/tsljgj/AQAEnv) pipeline, which:
1. Takes seed Q&A pairs and progressively expands them through multiple reasoning levels
2. Validates each expansion step with web search and code execution tools
3. Runs a two-tier verification system to ensure answer correctness

---

## Scoring System

### Weighted Score

The weighted score accounts for question difficulty:

```
weighted_score = Σ(weight × pass_rate × num_questions) / Σ(weight × num_questions)
```

Where weights are:
- Easy: 1x
- Medium: 2x
- Hard: 3x
- Expert: 4x

### LLM Evaluation

Answers are evaluated using GPT-4o-mini with semantic equivalence checking:
- Exact matches → score: 1.0
- Semantically equivalent → score: 1.0 (e.g., "Paris" ≡ "Paris, France")
- Partial answers → score: 0.0-1.0
- Wrong answers → score: 0.0

---

## Project Structure

```
counterfacts-green-agent/
├── src/
│   ├── server.py          # A2A server & agent card
│   ├── agent.py           # Core benchmark evaluator
│   ├── dataset.py         # Dataset loader & sampler
│   ├── evaluator.py       # LLM-as-judge evaluator
│   ├── executor.py        # A2A request executor
│   └── messenger.py       # A2A messaging utilities
├── data/
│   └── data_ready/        # 167 benchmark questions (JSON)
├── test-purple-agent/     # Sample purple agent for testing
│   ├── src/
│   │   ├── server.py      # Purple agent server
│   │   ├── agent.py       # Simple QA agent using GPT
│   │   └── executor.py    # A2A executor
│   ├── Dockerfile
│   └── pyproject.toml
├── tests/
│   ├── test_agent.py      # A2A conformance tests
│   └── conftest.py        # Pytest fixtures
├── Dockerfile             # Container configuration
├── docker-compose.yml     # Multi-container setup (green + purple)
├── scenario.toml          # Local testing scenario config
├── pyproject.toml         # Dependencies & metadata
├── test_interaction.py    # Integration test script
└── README.md
```

---

## Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `OPENAI_API_KEY` | Yes | - | OpenAI API key for LLM evaluation |
| `OPENAI_BASE_URL` | No | OpenAI default | Custom OpenAI-compatible API endpoint |

---

## CI/CD

The repository includes a GitHub Actions workflow that:

1. **On Pull Request**: Builds Docker image, runs tests
2. **On Push to main**: Builds, tests, and publishes `latest` tag
3. **On Version Tag** (e.g., `v1.0.0`): Publishes versioned image

Published images are available at:
```
ghcr.io/tsljgj/counterfacts-green-agent:latest
ghcr.io/tsljgj/counterfacts-green-agent:1.0.0
```

---

## Related Projects

- **[AQAEnv](https://github.com/tsljgj/AQAEnv)** - The data curation pipeline that generates the benchmark questions
- **[A2A Protocol](https://a2a-protocol.org/)** - The agent-to-agent communication protocol
- **[AgentBeats](https://agentbeats.dev)** - Platform for agent benchmarking competitions

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- Dataset generated using the [AQAEnv](https://github.com/tsljgj/AQAEnv) pipeline
- Built with the [A2A SDK](https://github.com/a2a-sdk/a2a-sdk)
- LLM evaluation powered by OpenAI

---

<p align="center">
  <b>CounterFacts Green Agent</b> - Evaluating AI reasoning capabilities
</p>
